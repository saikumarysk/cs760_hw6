\documentclass[a4paper]{article}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{paralist}
\usepackage{epstopdf}
\usepackage{tabularx}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{multicol}
\usepackage[hidelinks]{hyperref}
\usepackage{fancyvrb}
\usepackage{float}
\usepackage{paralist}
\usepackage[svgname]{xcolor}
\usepackage{enumerate}
\usepackage{array}
\usepackage{times}
\usepackage{url}
\usepackage{fancyhdr}
\usepackage{comment}
\usepackage{environ}
\usepackage{times}
\usepackage{textcomp}
\usepackage{caption}
\usepackage{bbm}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tikz}


\urlstyle{rm}

\setlength\parindent{0pt} % Removes all indentation from paragraphs
\theoremstyle{definition}
\newtheorem{definition}{Definition}[]
\newtheorem{conjecture}{Conjecture}[]
\newtheorem{example}{Example}[]
\newtheorem{theorem}{Theorem}[]
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}

\floatname{algorithm}{Procedure}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\Nat}{\mathbb{N}}
\newcommand{\br}[1]{\{#1\}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\renewcommand{\qedsymbol}{$\blacksquare$}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}

\newcommand{\vc}[1]{\boldsymbol{#1}}
\newcommand{\xv}{\vc{x}}
\newcommand{\Sigmav}{\vc{\Sigma}}
\newcommand{\alphav}{\vc{\alpha}}
\newcommand{\muv}{\vc{\mu}}

\newcommand{\red}[1]{\textcolor{red}{#1}}

\def\x{\mathbf x}
\def\y{\mathbf y}
\def\w{\mathbf w}
\def\v{\mathbf v}
\def\E{\mathbb E}
\def\V{\mathbb V}
\def\ind{\mathbbm 1}

% TO SHOW SOLUTIONS, include following (else comment out):
\newenvironment{soln}{
	\leavevmode\color{blue}\ignorespaces
}{}

\hypersetup{
	%    colorlinks,
	linkcolor={red!50!black},
	citecolor={blue!50!black},
	urlcolor={blue!80!black}
}

\geometry{
	top=1in,            % <-- you want to adjust this
	inner=1in,
	outer=1in,
	bottom=1in,
	headheight=3em,       % <-- and this
	headsep=2em,          % <-- and this
	footskip=3em,
}


\pagestyle{fancyplain}
\lhead{\fancyplain{}{Homework 6}}
\rhead{\fancyplain{}{CS 760 Machine Learning}}
\cfoot{\thepage}

\title{\textsc{Homework 6}} % Title

%%% NOTE:  Replace 'NAME HERE' etc., and delete any "\red{}" wrappers (so it won't show up as red)

\author{
	Saikumar Yadugiri\\
	9083079468, saikumar@cs.wisc.edu\\
} 

\date{}

\begin{document}
	
	\maketitle 
	
        \textbf{Instructions:}
        Use this latex file as a template to develop your homework. Submit your homework on time as a single pdf file. Please wrap your code and upload to a public GitHub repo, then attach the link below the instructions so that we can access it. Answers to the questions that are not within the pdf are not accepted. This includes external links or answers attached to the code implementation. Late submissions may not be accepted. You can choose any programming language (i.e. python, R, or MATLAB). Please check Piazza for updates about the homework. It is ok to share the results of the experiments and compare them with each other.

        \paragraph{GitHub Link:} \href{https://github.com/saikumarysk/cs760_hw6}{\texttt{https://github.com/saikumarysk/cs760\_hw6}}
	
	\section{Implementation: GAN (50 pts)}
	In this part, you are expected to implement GAN with MNIST dataset. We have provided a base jupyter notebook (gan-base.ipynb) for you to start with, which provides a model setup and training configurations to train GAN with MNIST dataset.
	
	\begin{enumerate} [label=(\alph*)]
		\item Implement training loop and report learning curves and generated images in epoch 1, 50, 100. Note that drawing learning curves and visualization of images are already implemented in provided jupyter notebook. \hfill (20 pts)
		
		\begin{algorithm}
			\caption{Training GAN, modified from \cite{goodfellow2014generative}}\label{alg:GAN}
			\begin{algorithmic}
				\Require $m$: real data batch size, $n_{z}$: fake data batch size
				\Ensure Discriminator $D$, Generator $G$
				
				\For{number of training iterations}
				
				\State{$\#$ Training discriminator}
				\State{Sample minibatch of $n_{z}$ noise samples $\{z^{(1)}, z^{(2)}, \cdots, z^{(n_{z})}\}$ from noise prior $p_{g}(z)$}
				\State{Sample minibatch of $\{x^{(1)}, x^{(2)}, \cdots, x^{(m)}\}$}
				\State{Update the discriminator by ascending its stochastic  gradient:
					$$\nabla_{\theta_{d}} \big ( \cfrac{1}{m}  \sum_{i=1}^{m}  \log D(x^{(i)})  + \cfrac{1}{n_{z}} \sum_{i=1}^{n_{z}}  \log (1-D(G(z^{(i)})))\big )$$
				}
				
				\State{$\#$ Training generator}
				\State{Sample minibatch of $n_{z}$ noise samples $\{z^{(1)}, z^{(2)}, \cdots, z^{(n_{z})}\}$ from noise prior $p_{g}(z)$}
				\State{Update the generator  by ascending its stochastic  gradient:
					$$\nabla_{\theta_{g}}  \cfrac{1}{n_{z}} \sum_{i=1}^{n_{z}}  \log D(G(z^{(i)}))\big )$$
				}
				\EndFor
				
				\State{$\#$ The gradient-based updates can use any standard gradient-based learning rule. In the base code, we are using Adam optimizer \citep{kingma2014adam}}
			\end{algorithmic}
		\end{algorithm}
		
		Expected results are as follows.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.7\textwidth]{gan_q1_loss.png}
			\caption{Learning curve}
			\label{fig:gan_q1_loss}
		\end{figure}
		
		\begin{figure}[H]
			\centering
			\begin{subfigure}[b]{0.3\textwidth}
				\centering
				\includegraphics[width=\textwidth]{gan_q1_epoch1.png}
				\caption{epoch 1}
			\end{subfigure}
			\hfill
			\begin{subfigure}[b]{0.3\textwidth}
				\centering
				\includegraphics[width=\textwidth]{gan_q1_epoch50.png}
				\caption{epoch 50}
			\end{subfigure}
			\hfill
			\begin{subfigure}[b]{0.3\textwidth}
				\centering
				\includegraphics[width=\textwidth]{gan_q1_epoch100.png}
				\caption{epoch 100}
			\end{subfigure}
			\caption{Generated images by $G$}
			\label{fig:three graphs}
		\end{figure}
		
		
		\begin{soln}
            My implementation results are shown in figures \ref{fig:1} and \ref{fig:2}. Specifically, the loss function is shown in \ref{fig:1} and the epochs are shown in \ref{fig:2}.

            \begin{figure}[H]
			\centering
			\includegraphics[width=0.7\textwidth]{loss_q1.png}
			\caption{Learning curve}
			\label{fig:1}
		\end{figure}
		
		\begin{figure}[H]
			\centering
			\begin{subfigure}[b]{0.3\textwidth}
				\centering
				\includegraphics[width=\textwidth]{epoch_5_q1.png}
				\caption{epoch 1}
			\end{subfigure}
			\hfill
			\begin{subfigure}[b]{0.3\textwidth}
				\centering
				\includegraphics[width=\textwidth]{epoch_50_q1.png}
				\caption{epoch 50}
			\end{subfigure}
			\hfill
			\begin{subfigure}[b]{0.3\textwidth}
				\centering
				\includegraphics[width=\textwidth]{epoch_100_q1.png}
				\caption{epoch 100}
			\end{subfigure}
			\caption{Generated images by $G$}
			\label{fig:2}
		\end{figure}
        \end{soln}
		
		
		
		\item Replace the generator update rule as the original one in the slide,\\
		``Update the generator by descending its stochastic gradient:
		
		$$\nabla_{\theta_{g}}  \cfrac{1}{n_{z}}  \sum_{i=1}^{n_{z}}\log (1-D(G(z^{(i)})))\big )$$
		"
		, and report learning curves and generated images in epoch 1, 50, 100. Compare the result with (a). Note that it may not work. If training does not work, explain why it doesn't work. \\
        You may find this helpful: https://jonathan-hui.medium.com/gan-what-is-wrong-with-the-gan-cost-function-6f594162ce01
		\hfill (10 pts)
		
		\begin{soln}
            My implementation results are shown in figures \ref{fig:3} and \ref{fig:4}. Specifically, the loss function is shown in \ref{fig:3} and the epochs are shown in \ref{fig:4}.

            The impression I have about why this doesn't work is that this rule has a ``vanshing gradient'' issue. As $D(G(z))$ is near 0, SGD cannot step too much limiting the capability of learning of the Generator.

            \begin{figure}[H]
			\centering
			\includegraphics[width=0.7\textwidth]{loss_q2.png}
			\caption{Learning curve}
			\label{fig:3}
		\end{figure}
		
		\begin{figure}[H]
			\centering
			\begin{subfigure}[b]{0.3\textwidth}
				\centering
				\includegraphics[width=\textwidth]{epoch_5_q2.png}
				\caption{epoch 1}
			\end{subfigure}
			\hfill
			\begin{subfigure}[b]{0.3\textwidth}
				\centering
				\includegraphics[width=\textwidth]{epoch_50_q2.png}
				\caption{epoch 50}
			\end{subfigure}
			\hfill
			\begin{subfigure}[b]{0.3\textwidth}
				\centering
				\includegraphics[width=\textwidth]{epoch_100_q2.png}
				\caption{epoch 100}
			\end{subfigure}
			\caption{Generated images by $G$}
			\label{fig:4}
		\end{figure}
        \end{soln}
		
		\item Except the method that we used in (a), how can we improve training for GAN? Implement that and report your setup, learning curves, and generated images in epoch 1, 50, 100.
        This question is an open-ended question and you can choose whichever method you want.
		\hfill (20 pts)
		
		\begin{soln}
            I changed the batch size to 128 to make the curve a lot smoother. I also used 0.9 for the discriminator's real label instead of 1 so that the network will not suffer from overconfidence. I tried a different torch seed as well. The implementation results are shown in figures \ref{fig:5} and \ref{fig:6}. Specifically, the loss function is shown in \ref{fig:5} and the epochs are shown in \ref{fig:6}. Visual improvement can be seen in the process.

            \begin{figure}[H]
			\centering
			\includegraphics[width=0.7\textwidth]{loss_q3.png}
			\caption{Learning curve}
			\label{fig:5}
		\end{figure}
		
		\begin{figure}[H]
			\centering
			\begin{subfigure}[b]{0.3\textwidth}
				\centering
				\includegraphics[width=\textwidth]{epoch_5_q3.png}
				\caption{epoch 1}
			\end{subfigure}
			\hfill
			\begin{subfigure}[b]{0.3\textwidth}
				\centering
				\includegraphics[width=\textwidth]{epoch_50_q3.png}
				\caption{epoch 50}
			\end{subfigure}
			\hfill
			\begin{subfigure}[b]{0.3\textwidth}
				\centering
				\includegraphics[width=\textwidth]{epoch_100_q3.png}
				\caption{epoch 100}
			\end{subfigure}
			\caption{Generated images by $G$}
			\label{fig:6}
		\end{figure}
        \end{soln}
		
	\end{enumerate}

\pagebreak

\section{Directed Graphical Model [25 points]}
Consider the directed graphical model (aka Bayesian network) in Figure~\ref{fig:bn}.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{BN.jpeg}
    \caption{A Bayesian Network example.}
    \label{fig:bn}
\end{figure}
Compute $P(B=t \mid E=f,J=t,M=t)$ and $P(B=t \mid E=t,J=t,M=t)$. (10 points for each) These are the conditional probabilities of a burglar in your house (yikes!) when both of your neighbors John and Mary call you and say they hear an alarm in your house, but without or with an earthquake also going on in that area (what a busy day), respectively.

\begin{soln}
Using Bayes' theorem,

\begin{equation*}
    \Pr [B = t | E = f, J = t, M = t] = \frac{\Pr [E = f, J = t, M = t | B = t] \Pr [B = t]}{\Pr [E = f, J = t, M = t]}
\end{equation*}

From the law of total probability, \\
$\Pr [E = f, J = t, M = t] = \Pr [E = f, J = t, M = t | B = t] \Pr [B = t] + \Pr [E = f, J = t, M = t | B = f] \Pr [B = f]$.\\
$\Pr[E = f, J = t, M = t | B = t] = \Pr[J = t, M = t | E = f, B = t] \Pr [E = f]$.\\
$= \sum\limits_{a \in \{t, f\}} \Pr[J = t, M = t | A = a]\Pr[A = a | E = f, B = t] \Pr [E = f] = ((0.9 \times 0.7) \times 0.8 + (0.2 \times 0.1) \times 0.2) \times 0.8 = 0.4064$.

Similarly, $\Pr[E = f, J = t, M = t | B = t] = ((0.9 \times 0.7) \times 0.1 + (0.2 \times 0.1) \times 0.9) \times 0.8 = 0.0648$.

Hence, $\Pr [E = f, J = t, M = t] = 0.4064 \times 0.1 + 0.0648 \times 0.9 = 0.09896$.\\
$\therefore \Pr [B = t | E = f, J = t, M = t] = \frac{0.4064}{0.09896} = \mathbf{0.4106}$.

Following the same procedure above,

\begin{equation*}
    \Pr [B = t | E = t, J = t, M = t] = \frac{\Pr [E = t, J = t, M = t | B = t] \Pr [B = t]}{\Pr [E = t, J = t, M = t]}
\end{equation*}
And, $\Pr [E = t, J = t, M = t] = \sum\limits_{a \in \{t, f\}} \sum\limits_{b \in \{t, f\}} \Pr[J = t, M = t | A = a] \Pr[A = a | B = b, E = t] \Pr[E = t] \Pr[B = b] = 0.01138 + 0.03654 = 0.0479$.\\
$\therefore \Pr[B = t | E = t, J = t, M = t] = \frac{0.01138}{0.0479} = \mathbf{0.237}$.
\end{soln}

\pagebreak


\section{Chow-Liu Algorithm [25 pts]}
Suppose we wish to construct a directed graphical model for 3 features $X$, $Y$, and $Z$ using the Chow-Liu algorithm. We are given data from 100 independent experiments where each feature is binary and takes value $T$ or $F$. Below is a table summarizing the observations of the experiment:

\begin{table}[H]
        \centering
                \begin{tabular}{cccc}
                           $X$ & $Y$ & $Z$ & Count \\
                                \hline
                                T & T & T & 36 \\
                                \hline
                                T & T & F & 4 \\
                                \hline
                                T & F & T & 2 \\
                                \hline
                                T & F & F & 8 \\
                                \hline
                                F & T & T & 9 \\
                                \hline
                                F & T & F & 1 \\
                                \hline
                                F & F & T & 8 \\
                                \hline
                                F & F & F & 32 \\
                                \hline
                \end{tabular}
\end{table}

\begin{enumerate}
	\item Compute the mutual information $I(X, Y)$ based on the frequencies observed in the data. (5 pts) \\
    \begin{soln}
        $I(X, Y) = H_D(X) - H_D(X | Y)$.\\
        $H_D (X) = -\Pr[X = t] \log_2(\Pr[X = t]) - \Pr[X = f] \log_2 (\Pr [X = f]) = - \frac{50}{100} \log_2 \left( \frac{50}{100} \right) - \frac{50}{100} \log_2 \left( \frac{50}{100} \right) = 1$.

        $H_D(X | Y = t) = - \frac{40}{50} \log_2 \left( \frac{40}{50} \right) - \frac{10}{50} \log_2 \left( \frac{10}{50} \right) = 0.722$ \\
        $H_D(X | Y = f) = - \frac{10}{50} \log_2 \left( \frac{10}{50} \right) - \frac{40}{50} \log_2 \left( \frac{40}{50} \right) = 0.722$ \\

        $H_D(X | Y) = 0.5 (0.722) + 0.5 (0.722) = 0.722$. Hence, $I(X, Y) = 1 - 0.722 = \mathbf{0.278}$
        
    \end{soln}
	\item Compute the mutual information $I(X, Z)$ based on the frequencies observed in the data. (5 pts) \\
    \begin{soln}
        Similar to above, we have that $I(X, Z) = \mathbf{0.125}$.
    \end{soln}
	\item Compute the mutual information $I(Z, Y)$ based on the frequencies observed in the data. (5 pts) \\
    \begin{soln}
        Similar to part 1, we can calculate that $I(Z, Y) = \mathbf{0.3916}$.
    \end{soln}
	\item Which undirected edges will be selected by the Chow-Liu algorithm as the maximum spanning tree? (5 pts) \\
    \begin{soln}
        Using Kruskal's algorithm for maximum spanning tree, we select the two nodes with maximum edge weight. Here, the edge weight used is the mutual information between nodes. As the mutual information between the $Y, Z$ and $X, Y$ is the biggest two, we will select these edges.
    \end{soln}
	\item Root your tree at node $X$, assign directions to the selected edges. (5 pts) \\
    \begin{soln}
        The tree is a simple path tree as shown in figure \ref{fig:tree}.
        \begin{figure}[h]
            \centering
            \begin{tikzpicture}
                \node (X) at (0, 0) {$X$};
                \node (Y) at (0, -1) {$Y$};
                \node (Z) at (0, -2) {$Z$};

                \draw (0, 0) circle (0.26cm);
                \draw (0, -1) circle (0.26cm);
                \draw (0, -2) circle (0.26cm);

                \draw[-latex] (X) -- (Y);
                \draw[-latex] (Y) -- (Z);
            \end{tikzpicture}
            \caption{Directed Tree from Chow-Liu's Algorithm}
            \label{fig:tree}
        \end{figure}
    \end{soln}
\end{enumerate}

	\bibliography{hw6}
	\bibliographystyle{apalike}
\end{document}